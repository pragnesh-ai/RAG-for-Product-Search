{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/ImageBind\n# cd ImageBind","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-30T12:00:57.754556Z","iopub.execute_input":"2024-04-30T12:00:57.754962Z","iopub.status.idle":"2024-04-30T12:00:59.944389Z","shell.execute_reply.started":"2024-04-30T12:00:57.754913Z","shell.execute_reply":"2024-04-30T12:00:59.943022Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'ImageBind'...\nremote: Enumerating objects: 117, done.\u001b[K\nremote: Counting objects: 100% (70/70), done.\u001b[K\nremote: Compressing objects: 100% (35/35), done.\u001b[K\nremote: Total 117 (delta 48), reused 35 (delta 35), pack-reused 47\u001b[K\nReceiving objects: 100% (117/117), 2.64 MiB | 15.27 MiB/s, done.\nResolving deltas: 100% (53/53), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd ImageBind","metadata":{"execution":{"iopub.status.busy":"2024-04-30T12:00:59.947163Z","iopub.execute_input":"2024-04-30T12:00:59.947641Z","iopub.status.idle":"2024-04-30T12:00:59.956050Z","shell.execute_reply.started":"2024-04-30T12:00:59.947587Z","shell.execute_reply":"2024-04-30T12:00:59.954789Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/ImageBind\n","output_type":"stream"}]},{"cell_type":"code","source":"\n!pip install -r requirements.txt\n!pip install soundfile","metadata":{"execution":{"iopub.status.busy":"2024-04-30T12:00:59.957532Z","iopub.execute_input":"2024-04-30T12:00:59.958030Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d (from -r requirements.txt (line 4))\n  Cloning https://github.com/facebookresearch/pytorchvideo.git (to revision 28fe037d212663c6a24f373b94cc5d478c8c1a1d) to /tmp/pip-install-vxq57crw/pytorchvideo_95d194c5841d455e8545a01694605174\n  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/pytorchvideo.git /tmp/pip-install-vxq57crw/pytorchvideo_95d194c5841d455e8545a01694605174\n  Running command git rev-parse -q --verify 'sha^28fe037d212663c6a24f373b94cc5d478c8c1a1d'\n  Running command git fetch -q https://github.com/facebookresearch/pytorchvideo.git 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n  Running command git checkout -q 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n  Resolved https://github.com/facebookresearch/pytorchvideo.git to commit 28fe037d212663c6a24f373b94cc5d478c8c1a1d\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting torch==1.13.0 (from -r requirements.txt (line 1))\n  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\nCollecting torchvision==0.14.0 (from -r requirements.txt (line 2))\n  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\nCollecting torchaudio==0.13.0 (from -r requirements.txt (line 3))\n  Downloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (1.0 kB)\nCollecting timm==0.6.7 (from -r requirements.txt (line 5))\n  Downloading timm-0.6.7-py3-none-any.whl.metadata (33 kB)\nCollecting ftfy (from -r requirements.txt (line 6))\n  Downloading ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2023.12.25)\nCollecting einops (from -r requirements.txt (line 8))\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting fvcore (from -r requirements.txt (line 9))\n  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting eva-decord==0.6.1 (from -r requirements.txt (line 10))\n  Downloading eva_decord-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (449 bytes)\nCollecting iopath (from -r requirements.txt (line 11))\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.19 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (1.26.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (3.7.5)\nCollecting types-regex (from -r requirements.txt (line 14))\n  Downloading types_regex-2024.4.28.20240430-py3-none-any.whl.metadata (1.6 kB)\nCollecting mayavi (from -r requirements.txt (line 15))\n  Downloading mayavi-4.8.1.tar.gz (20.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.6/20.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: cartopy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (0.23.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==1.13.0->-r requirements.txt (line 1)) (4.9.0)\nCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.0->-r requirements.txt (line 1))\n  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0->-r requirements.txt (line 2)) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.14.0->-r requirements.txt (line 2)) (9.5.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->-r requirements.txt (line 1)) (69.0.3)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->-r requirements.txt (line 1)) (0.42.0)\nCollecting av (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 4))\n  Downloading av-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nCollecting parameterized (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 4))\n  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pytorchvideo@ git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d->-r requirements.txt (line 4)) (3.2.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->-r requirements.txt (line 6)) (0.2.13)\nCollecting yacs>=0.1.6 (from fvcore->-r requirements.txt (line 9))\n  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->-r requirements.txt (line 9)) (6.0.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from fvcore->-r requirements.txt (line 9)) (4.66.1)\nRequirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.10/site-packages (from fvcore->-r requirements.txt (line 9)) (2.4.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fvcore->-r requirements.txt (line 9)) (0.9.0)\nCollecting portalocker (from iopath->-r requirements.txt (line 11))\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 13)) (2.9.0.post0)\nCollecting apptools (from mayavi->-r requirements.txt (line 15))\n  Downloading apptools-5.2.1-py3-none-any.whl.metadata (4.2 kB)\nCollecting envisage (from mayavi->-r requirements.txt (line 15))\n  Downloading envisage-7.0.3-py3-none-any.whl.metadata (5.2 kB)\nCollecting pyface>=6.1.1 (from mayavi->-r requirements.txt (line 15))\n  Downloading pyface-8.0.0-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from mayavi->-r requirements.txt (line 15)) (2.17.2)\nCollecting traits>=6.0.0 (from mayavi->-r requirements.txt (line 15))\n  Downloading traits-6.4.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\nCollecting traitsui>=7.0.0 (from mayavi->-r requirements.txt (line 15))\n  Downloading traitsui-8.0.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: vtk in /opt/conda/lib/python3.10/site-packages (from mayavi->-r requirements.txt (line 15)) (9.3.0)\nRequirement already satisfied: shapely>=1.7 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 16)) (1.8.5.post1)\nRequirement already satisfied: pyshp>=2.3 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 16)) (2.3.1)\nRequirement already satisfied: pyproj>=3.3.1 in /opt/conda/lib/python3.10/site-packages (from cartopy->-r requirements.txt (line 16)) (3.6.1)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from pyproj>=3.3.1->cartopy->-r requirements.txt (line 16)) (2024.2.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 13)) (1.16.0)\nCollecting configobj (from apptools->mayavi->-r requirements.txt (line 15))\n  Downloading configobj-5.0.8-py2.py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 2)) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 2)) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.14.0->-r requirements.txt (line 2)) (1.26.18)\nDownloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0mm\n\u001b[?25hDownloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl (24.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-0.13.0-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading timm-0.6.7-py3-none-any.whl (509 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading eva_decord-0.6.1-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n\u001b[2K   \u001b[91m━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/317.1 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:01:06\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install qdrant-client>=1.1.1\n!pip install -U sentence-transformers\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install opendatasets gradio qdrant-client transformers sentence_transformers sentencepiece tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import opendatasets as od\nod.download(\"https://www.kaggle.com/datasets/sriramr/apples-bananas-oranges\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from qdrant_client import models, QdrantClient\nimport random\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport torch\nimport sys\nsys.path.append(\"./ImageBind/\")\nimport imagebind\nfrom imagebind.models import imagebind_model\nfrom imagebind.models.imagebind_model import ModalityType\n\nfrom imagebind import data\nimport uuid\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Qdrant client\nqdrant = QdrantClient(\":memory:\")\n\ndef get_image_paths(directory):\n    \"\"\"\n    Retrieve paths of images from the specified directory.\n\n    Args:\n    directory (str): The directory containing the images.\n\n    Returns:\n    list: List of image paths.\n    \"\"\"\n    image_paths = []\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                image_path = os.path.join(root, file)\n                image_paths.append(image_path)\n    return image_paths\n\n# Directory paths\napple_directory = './apples-bananas-oranges/original_data_set/freshapples/'\nbanana_directory = './apples-bananas-oranges/original_data_set/freshbanana'\norange_directory = './apples-bananas-oranges/original_data_set/freshoranges'\n\n# Get image paths for different categories\nimage_paths_apple = get_image_paths(apple_directory)\nimage_paths_banana = get_image_paths(banana_directory)\nimage_paths_orange = get_image_paths(orange_directory)\n\nall_image_paths = [image_paths_orange, image_paths_banana, image_paths_apple]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\nmodel = imagebind_model.imagebind_huge(pretrained=True)\nmodel.eval()\nmodel.to(device)\n\nembeddings_list = []\nfor image_paths in [image_paths_orange, image_paths_banana, image_paths_apple]:\n  inputs = {ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device)}\n  # print(inputs)\n  with torch.no_grad():\n    embeddings = model(inputs)\n    embeddings_list.append(embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import uuid\n# Initialize Qdrant client and load collection\nclient = QdrantClient(\":memory:\")\nclient.recreate_collection(collection_name = \"imagebind_data\",\nvectors_config = {\"image\": models.VectorParams( size = 1024, distance = models.Distance.COSINE ) } )\npoints = []\n# Iterate over each embeddings and corresponding image paths\nfor idx, (embedding, image_paths) in enumerate(zip(embeddings, all_image_paths)):\n  # print(idx,embeddings['vision'],image_paths)\n  for sub_idx, sample in enumerate(image_paths):\n    # print(image_paths)\n  # Convert the sample to a dictionary\n    payload = {\"path\": sample}\n    # print(embedding)\n    # Generate a unique UUID for each point\n    point_id = str(uuid.uuid4())\n    points.append(models.PointStruct(id=point_id,\n    vector= {\"image\": embeddings['vision'][sub_idx]},\n    payload=payload)\n    )\nclient.upsert(collection_name=\"imagebind_data\", points=points)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install opencv-python ultralytics\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\n\nyolo_model = YOLO(\"yolov9c.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(chosen_model, img, classes=[], conf=0.5):\n    \"\"\"\n    Perform object detection on the input image.\n\n    Args:\n    chosen_model: YOLO model instance.\n    img: Input image.\n    classes: List of classes to detect (optional).\n    conf: Confidence threshold for detections (optional).\n\n    Returns:\n    results: Detection results.\n    \"\"\"\n    if classes:\n        results = chosen_model.predict(img, classes=classes, conf=conf)\n    else:\n        results = chosen_model.predict(img, conf=conf)\n\n    return results\n\ndef predict_and_detect(chosen_model, img, classes=[], conf=0.5, rectangle_thickness=2, text_thickness=1):\n    \"\"\"\n    Perform object detection on the input image and save cropped images of detected objects.\n\n    Args:\n    chosen_model: YOLO model instance.\n    img: Input image.\n    classes: List of classes to detect (optional).\n    conf: Confidence threshold for detections (optional).\n    rectangle_thickness: Thickness of rectangle around detected objects (optional).\n    text_thickness: Thickness of text overlay on detected objects (optional).\n\n    Returns:\n    img: Image with detections overlayed.\n    results: Detection results.\n    \"\"\"\n    results = predict(chosen_model, img, classes, conf=conf)\n    for result in results:\n        for box in result.boxes:\n            cropped_object = img[int(box.xyxy[0][1]):int(box.xyxy[0][3]), int(box.xyxy[0][0]):int(box.xyxy[0][2])]\n            cv2.imwrite(f\"{result.names[int(box.cls[0])]}\"+\".jpg\", cropped_object)\n\n    return img, results","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread(\"/kaggle/working/ImageBind/apples-bananas-oranges/original_data_set/rottenoranges/Screen Shot 2018-06-12 at 11.26.07 PM.png\")\nresult_img, _ = predict_and_detect(yolo_model, image, classes=[], conf=0.3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gradio as gr\nimport gradio\nimport cv2\nimport base64\nimport numpy as np\nfrom PIL import Image\nfrom qdrant_client import models, QdrantClient\nfrom ultralytics import YOLO\nimport torch\n\n# Initialize Qdrant client\n# client = QdrantClient(\":memory:\")\n\n# Initialize YOLO model\n# yolo_model = YOLO(\"yolov5s.pt\")\n\n# Function to perform object detection on the input image\ndef detect_objects(image):\n    # Convert image to numpy array\n    img_array = np.array(image)\n    # Perform object detection using YOLO\n    results = yolo_model.predict(img_array)\n    # Convert the results to PIL format\n    result_images = [Image.fromarray(obj) for obj in results.imgs]\n    # Return the detected objects\n    return result_images\n\n# Function to search for similar objects in Qdrant database\ndef search_similar_objects(detected_objects):\n    similar_images = []\n    for obj in detected_objects:\n        # Convert the detected object image to base64\n        _, img_encoded = cv2.imencode('.jpg', cv2.cvtColor(np.array(obj), cv2.COLOR_RGB2BGR))\n        img_base64 = base64.b64encode(img_encoded).decode('utf-8')\n        \n        # Convert the base64 image to vector format\n        obj_vector = image_to_vector(obj)\n        \n        # Search for similar objects in the Qdrant database\n        object_hits = client.search(\n            collection_name='imagebind_data',\n            query_vector=models.NamedVector(\n                name=\"object\",\n                vector=obj_vector.tolist()\n            )\n        )\n        \n        # Add the search results to the list of similar images\n        if object_hits:\n            for hit in object_hits:\n                similar_images.append(hit.payload['image_base64'])\n    \n    # Return the list of similar images\n    return similar_images\n\n# Function to convert image to vector using ImageBind model\ndef image_to_vector(image):\n    # Load and transform the image data\n    image_data = {\"image\": data.load_and_transform_vision_data([image], device)}\n    \n    # Generate embeddings for the image\n    with torch.no_grad():\n        embeddings = model(image_data)\n    \n    # Extract the vision embeddings\n    vision_embeddings = embeddings['vision']\n    \n    # Return the embeddings as a vector\n    return vision_embeddings[0].tolist()\n\n# Gradio Interface\niface = gr.Interface(\n    fn=detect_objects,\n    inputs=[gr.Image(label=\"image_query\", type=\"filepath\")],\n    outputs=[\n        gr.Image(label=\"Image\")],\n    title=\"Object Detection and Similar Object Search\",\n    description=\"Upload an image to detect objects and find similar objects using ImageBind and Qdrant.\",\n)\n\niface.launch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U gradio\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}